{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2769e4e",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3941835",
   "metadata": {},
   "source": [
    "In this notebook, we will begin by installing and importing the required libraries for our exploration of Neural Networks (NN). Before we delve into the intricacies of training a NN, we will first explore various activation functions, which play a crucial role in determining the output of each neuron in the network.\n",
    "\n",
    "**NB!**\n",
    "    ðŸ“‚ Note:\n",
    "    To run this notebook successfully, make sure you download the required data files from the GitHub repository: https://github.com/lmoranglez/SummerSchool.git and save them in the same folder as this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell once to install the required libraries\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install pandas==2.2.3 scikit-learn==1.6.1 matplotlib==3.10.1 seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to import\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361f892-a9dc-418f-b454-c4350bc1bc99",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Activation Functions\n",
    "\n",
    "Neural Networks are powerful ML models. A key component of NNs is the **activation function**, which introduces non-linearity, enabling the network to learn complex patterns.\n",
    "\n",
    "In this Part 1, we will:\n",
    "- Explore common activation functions in NNs.\n",
    "- Visualize how they transform input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ed56d-83e4-423f-a0a1-0e382bc205d7",
   "metadata": {},
   "source": [
    "As shown in the presentation, the dot product of XÂ·W plus b (featuresÂ·weights + bias) results in a constant value, ressembling a multiple linear regression. \n",
    "$$\n",
    "  v =  w_1x_1 +  w_2x_2 + ... + w_mx_m + b = \\sum^m_{i=0} w_ix_i + b\n",
    "$$\n",
    "However, NN may require non-linearity to model complex patterns. This is where the **activation function** comes into play. By applying an **activation function** to $v$, we break linearity and enable the network to learn beyond simple linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c18952-4a2a-4aeb-b356-90af76f4a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear space\n",
    "x = np.linspace(-10, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabcd79-6884-4c2c-babe-27c603299d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def leaky_relu(x, alpha=0.1): return np.where(x > 0, x, alpha*x)\n",
    "def tanh(x): return np.tanh(x)\n",
    "def softplus(x): return np.log(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19c91e-8e1c-459a-a844-8bda502553f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a value for v between -10 and 10 to visualize the transformation with the activation functions\n",
    "v = 4  # Example pre-activation value (dot product + bias)\n",
    "print(f'Changes when the activation functions are applied over v : {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303937c9-7840-450f-a208-55270a6b65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of (function, color, name) for each activation\n",
    "activations = [\n",
    "    (sigmoid, 'blue', 'Sigmoid'),\n",
    "    (relu, 'green', 'ReLU'),\n",
    "    (leaky_relu, 'orange', 'Leaky ReLU'),\n",
    "    (tanh, 'purple', 'Tanh'),\n",
    "    (softplus, 'brown', 'Softplus')\n",
    "]\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.suptitle(\"Activation Functions\", fontsize=16, y=1.02)\n",
    "\n",
    "# Original data (linear transformation)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(x, x, color='red', label=\"Linear (No Activation)\")\n",
    "plt.axvline(v, color='black', linestyle='--', alpha=0.5)\n",
    "plt.axhline(v, color='black', linestyle='--', alpha=0.5)\n",
    "plt.scatter(v, v, color='black', s=100, label=f\"v = {v}\")\n",
    "plt.title(\"Linear Transformation\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Plot each activation\n",
    "for i, (func, color, name) in enumerate(activations, start=2):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.plot(x, func(x), color=color, label=name)\n",
    "    plt.scatter(x, func(x), color=color, s=10)\n",
    "    plt.axvline(v, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(func(v), color='black', linestyle='--', alpha=0.5)\n",
    "    plt.scatter(v, func(v), color='black', s=100, label=f\"{name}(v) = {func(v):.2f}\")\n",
    "    plt.title(f\"{name} Activation\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c6571-f5b2-4f5a-a1e6-d708349205ca",
   "metadata": {},
   "source": [
    "**Experimentation**: Adjust the value of `v` (between -10 and 10) and re-run the cells to observe how each activation function transforms it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13561d09-0a5e-4396-8cd7-3b4c9a76c277",
   "metadata": {},
   "source": [
    "# Part 2: Training a NN with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8e2bf-7910-440a-acab-64c8dd2bf0c9",
   "metadata": {},
   "source": [
    "Now that we understand activation functions, letâ€™s train a vanilla NN, a Multi-Layer Perceptron (MLP), using `scikit-learn` library.\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "    2.1. Load and get familiar with the dataset\n",
    "    2.2. Preprocess the dataset\n",
    "    2.3. Split data into input features (X) and target properties (y)\n",
    "    2.4. Standarize the data set.\n",
    "    2.5. Define the NN architecture and its hyperparameters\n",
    "    2.6. Train the NN model\n",
    "    2.7. Analyze the model performance\n",
    "    2.8. Check for outliers\n",
    "    2.9. Assess the evolution of the training procedure\n",
    "\n",
    "**Goal** \\\n",
    "Predict a target propertyâ€”either activation energies, bond distances, or anglesâ€”for derivatives of Vaska's complex (trans-[Ir(CO)Cl(PPhâ‚ƒ)â‚‚]) using computed chemical descriptors as input features to train predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49837504",
   "metadata": {},
   "source": [
    "### 2.1. Load and get familiar with the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e963caa",
   "metadata": {},
   "source": [
    "**Dataset** \n",
    "\n",
    "- Key features (Input, $X$): electronic structure parameters (e.g., orbital energies, partial charges), distances, atomic numbers...\n",
    "- Target variables (Output, $y$): 1) energy barriers (Î”Eâ€¡, in kcal/mol) for oxidative addition reactions; 2) HÂ·Â·Â·H bond distances (d(HÂ·Â·Â·H) in Ã…); 3) angles (angle(H-Ir-H) in Âº) of the dihydrogen activation in the Vaska's complexes (published in 2020, 2024).\n",
    "[data_Vaska](https://github.com/uiocompcat/AABBA/blob/main/data_Vaska.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba57957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the .csv file\n",
    "df = pd.read_csv(\"NN_Vaskawithoutlier.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target properties\n",
    "distance = df['target_distance'].values\n",
    "barrier = df['target_barrier'].values\n",
    "angle = df['target_angle'].values\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)    # nrows, ncols, index\n",
    "plt.hist(barrier, bins=20, color='purple', alpha=0.7)\n",
    "plt.title('Barrier Distribution')\n",
    "plt.xlabel('E (kcal/mol)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)    \n",
    "plt.hist(distance, bins=20, color='skyblue', alpha=0.7)\n",
    "plt.title('HÂ·Â·Â·H Distance Distribution')\n",
    "plt.xlabel('d(HÂ·Â·Â·H) (Ã…)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(angle, bins=20, color='grey', alpha=0.7)\n",
    "plt.title('HÂ·Â·Â·IrÂ·Â·Â·H Angle Distribution')\n",
    "plt.xlabel('Î¸ (degrees)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Mean and standard deviation of the target properties\n",
    "print(f\"Mean barrier: {np.mean(barrier):.2f} Â± {np.std(barrier):.2f} kcal/mol\")\n",
    "print(f\"Mean HÂ·Â·Â·H distance: {np.mean(distance):.2f} Â± {np.std(distance):.2f} Ã…\")\n",
    "print(f\"Mean HÂ·Â·Â·IrÂ·Â·Â·H angle: {np.mean(angle):.2f} Â± {np.std(angle):.2f} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee6545-a0f9-4519-a17d-cf554486154d",
   "metadata": {},
   "source": [
    "### 2.2. Preprocess the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c485eb0-64b7-4352-8f40-639c3b6c03fd",
   "metadata": {},
   "source": [
    "**NB!** Data treatment is one of the most demanding aspects when applying ML. It is important to be mindful of the `type` of input we introduce. Always refer to the documentation webpage to understand the input data format required in the functions.\n",
    "\n",
    "Let's split first the data into input features ($X$) and target feature ($y$). \n",
    "The three target properties $y$s were already separated as `barrier`, `distance` and `angle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d7eb3-7464-451c-a502-18af9480c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns in the dataframe\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cec4d-01c9-4f21-b6e6-1d5e97a5b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the id and target features from the df to define X\n",
    "df_features = df.drop(columns=['id',\n",
    "                            'target_distance', \n",
    "                            'target_barrier', \n",
    "                            'target_angle'])\n",
    "X = df_features.values\n",
    "\n",
    "# NB! Be sure that the X, y are a sequence of indexables with the same length / shape[0] = nrows\n",
    "print('Track the size of the data set', \n",
    "      'X input:', X.shape, \n",
    "      'distance:', distance.shape, 'barrier:', barrier.shape, 'angle:', angle.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd4d9f",
   "metadata": {},
   "source": [
    "### 2.3. Split data into input features (X) and target properties (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69d7e9-56fb-434f-b4fb-95c0545370da",
   "metadata": {},
   "source": [
    "Split the data into train and test subsets. \\\n",
    "**NB!** In this implementation, validation is handled internally by scikit-learn and thus, no explicit split is needed.\n",
    "Function to use: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eff1e2-c2cb-44d0-a4eb-3c467e4255d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target property, the partition of the test_size and the seed to initialize the weigths and ensure reproducibility\n",
    "target_property = barrier\n",
    "test_percentage = 0.2\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split array into random train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    target_property, \n",
    "                                                    test_size=test_percentage, \n",
    "                                                    random_state=seed)\n",
    "print('Track the size of the data set', \n",
    "      'X train:', X_train.shape,\n",
    "      'X test:', X_test.shape, \n",
    "      'y train:', y_train.shape, \n",
    "      'y test:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b77f4-8b51-408f-941c-4025ffd7d816",
   "metadata": {},
   "source": [
    "### 2.4. Standarize the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4986050",
   "metadata": {},
   "source": [
    "Common approach is to standarize the dataset:\n",
    "$$\n",
    "z = (x-u) / s\n",
    "$$\n",
    "where $u$ is the mean, and $s$ the standard deviation \\\n",
    "**NB!** To prevent data leakage, we should only fit the scaler on the training data and then transform the test data with train_mean and train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   # class 'StandardScaler' creates the instance/object 'scaler'\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1e803-a70d-40a2-b269-1ea21870a261",
   "metadata": {},
   "source": [
    "### 2.5. Define the NN architecture and its hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d9f75",
   "metadata": {},
   "source": [
    "Create a the NN architecture using [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) model. We must carefully first consider the basic building blocks of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(MLPRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ba5fb-ce66-4cc1-bc4b-c5674f5292e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (128, 128) \n",
    "activation = 'relu'\n",
    "solver = 'adam'\n",
    "epochs = 200\n",
    "learning_rate = 'constant'\n",
    "learning_rate_init=0.0001\n",
    "random_state=2025\n",
    "validation_fraction=0.1\n",
    "early_stopping=True\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82552efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s begin by specifying our initial hyperparameters and their respective values.\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,  # number of neurons in the hidden layers   n of hidden layers \n",
    "    activation=activation, # activation function\n",
    "    solver=solver,  # optimization algorithm\n",
    "    max_iter=epochs,  # number of iterations (epochs) to train the model\n",
    "    learning_rate=learning_rate,  # learning rate type\n",
    "    learning_rate_init=learning_rate_init,  # initial learning rate\n",
    "    random_state=random_state,  # random seed for reproducibility\n",
    "    validation_fraction=validation_fraction, # by default\n",
    "    early_stopping=early_stopping,  # stop training when validation score is not improving\n",
    "    verbose=verbose,  # print progress messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11dff1",
   "metadata": {},
   "source": [
    "### 2.6. Train the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb7356-65a6-42f8-8a38-1aba137cb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the architecture is defined, the training can start\n",
    "mlp.fit(X_train_scaled, y_train)   # X: input features, y: target property "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44440c3a-cb6b-4d5e-a0a9-0f744d9e63d2",
   "metadata": {},
   "source": [
    "### 2.7 Analyze the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989959a1",
   "metadata": {},
   "source": [
    "There are two important aspects to consider: 1) the evolution of the loss function during the training and 2) the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f41c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Evolution of the loss function \n",
    "loss_curve = mlp.loss_curve_  # get the loss curve (training error) for each iteration\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "plt.title('Training (and Validation Loss)')\n",
    "plt.xlabel('Iteration')\n",
    "# Plotting the training loss on the first axis\n",
    "ax1.plot(loss_curve, label='Training Loss', color='blue')\n",
    "ax1.set_ylabel('Loss', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Add validation score (only if early_stopping=True)\n",
    "if mlp.early_stopping:\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(mlp.validation_scores_, label='Validation RÂ²', color='orange')\n",
    "    ax2.set_ylabel('Validation Score (RÂ²)', color='orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    ax2.legend(loc='center right')\n",
    "    print(f\"Final Validation RÂ²: {mlp.validation_scores_[-1]:.3f}\")\n",
    "else:\n",
    "    print(\"Validation scores not tracked (early_stopping=False)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd10f68-32d2-467a-8b46-246a19110d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Evaluate the model performance\n",
    "train_score = mlp.score(X_train_scaled, y_train)\n",
    "test_score = mlp.score(X_test_scaled, y_test)\n",
    "y_train_pred = mlp.predict(X_train_scaled)\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final train performance, Train RÂ²: {train_score:.3f} | Train MAE: {train_mae:.3f}\")\n",
    "print(f\"Final test performance, Test RÂ²: {test_score:.3f} | Test MAE: {test_mae:.3f}\")\n",
    "\n",
    "if mlp.early_stopping:\n",
    "    validation_score = mlp.validation_scores_  # get the validation score for each iteration\n",
    "    print(f\"Final validation performance, Val. RÂ²: {np.round(validation_score[-1], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a670c-cea5-40de-9ab7-338c10508ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_manual(y, y_pred):    \n",
    "    R2 = 1 - ((np.sum((y - y_pred) ** 2))/(np.sum((y - np.mean(y)) ** 2)))\n",
    "    return R2\n",
    "\n",
    "# Printing the predictions than with the training and test sets.\n",
    "def plot_predictions(y_train_pred, y_train, y_test_pred, y_test):\n",
    "    \n",
    "    min_value_train = np.min(y_train_pred) - 1\n",
    "    max_value_train = np.max(y_train_pred) + 1\n",
    "    min_value_test = np.min(y_test_pred) - 1\n",
    "    max_value_test = np.max(y_test_pred) + 1\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # Training data plot\n",
    "    plt.subplot(1, 2, 1)  # Define a proper 1x2 grid of subplots\n",
    "    plt.scatter(y_train_pred, y_train, c='blue', label='Training Data')\n",
    "    plt.plot([min_value_train, max_value_train], [min_value_train, max_value_train], \"r--\")\n",
    "    plt.text(min_value_train*5, min_value_train*1.2, f'RÂ² = {r2_score_manual(y_train, y_train_pred):.2f}', fontsize=12, color='black')\n",
    "    plt.xlabel('Predicted train values')\n",
    "    plt.ylabel('True train values')\n",
    "    plt.title('Training Prediction Performance')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)  # Change to the correct subplot index\n",
    "    plt.scatter(y_test_pred, y_test, c='#66c2a5', label='Test Data')\n",
    "    plt.plot([min_value_test, max_value_test], [min_value_test, max_value_test], \"r--\")\n",
    "    plt.text(min_value_test*5, min_value_test*1.2, f'RÂ² = {r2_score_manual(y_test, y_test_pred):.2f}', fontsize=12, color='black')\n",
    "    plt.xlabel('Predicted test values')\n",
    "    plt.ylabel('True test values')\n",
    "    plt.title('Test Prediction Performance')\n",
    "    plt.legend()\n",
    "\n",
    "    # Set same length axis ranges with 5% margin of max value\n",
    "    plt.subplots_adjust(wspace=0.3)  # Adjust spacing between subplots\n",
    "\n",
    "    print(f'Train RÂ²= {r2_score_manual(y_train, y_train_pred):.2f}, ', f'Test RÂ²= {r2_score_manual(y_test, y_test_pred):.2f}')\n",
    "\n",
    "    min_value_train, max_value_train, min_value_test, max_value_test = 0,0,0,0\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8236e71-8adb-4265-a2da-5c9e9bee65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_predictions(y_train_pred, y_train, y_test_pred, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c699f0-fe5f-47d0-89a0-33a162bb4a8b",
   "metadata": {},
   "source": [
    "### 2.8. Check for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfc85d-1589-46b9-92f7-a13532c32f76",
   "metadata": {},
   "source": [
    "It is crucial to visualize the results, as the presence of outliers can sometimes create a misleading impression of poor model training and subpar performance. The model may actually be functioning well, but outliers can obscure this reality in various evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59347d62-3017-4cda-8eb9-f041e26852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outlier(pred_list):\n",
    "    mean_pred = np.mean(pred_list)\n",
    "    threshold = 10 if np.array_equal(target_property, distance) else 50\n",
    "    for index, item in enumerate(pred_list):\n",
    "        if item < -mean_pred * threshold or item > mean_pred * threshold:\n",
    "            return mean_pred, item, index\n",
    "    return None, None, None  # Explicit return if no outl\n",
    "\n",
    "mean_pred, outlier, id_outlier = identify_outlier(y_test_pred)\n",
    "\n",
    "if outlier is not None and id_outlier is not None: \n",
    "    print('Target property mean value:', np.round(mean_pred, 2),\n",
    "          'Outlier value:', np.round(outlier, 3), \n",
    "          'Outlier index value:', id_outlier)\n",
    "    \n",
    "    #if outlier and id_outliter are not (None, None): \n",
    "    y_test_pred = np.delete(y_test_pred, id_outlier)\n",
    "    y_test = np.delete(y_test, id_outlier)\n",
    "    X_test_scaled = np.delete(X_test_scaled, id_outlier, axis=0)\n",
    "\n",
    "index = 0 \n",
    "item = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd5752-96dc-4360-b93c-90b2211f909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_test = plot_predictions(y_train_pred, y_train, y_test_pred, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15b9fc-c6a7-42b7-9bd3-a8d8ebbb7148",
   "metadata": {},
   "source": [
    "### 2.9. Assess the evolution of the training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a2881",
   "metadata": {},
   "source": [
    "The MLPRegressor in `scikit-learn` does not provide performance tracking during each epoch. One alternative is to save model after each epoch, allowing you to load and evaluate it later. This way, you can analyze its performance evolution at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bad174-ca9a-4655-8d4b-95a733666d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,  # number of neurons in the hidden layers   n of hidden layers \n",
    "    activation=activation, # activation function\n",
    "    solver=solver,  # optimization algorithm\n",
    "    learning_rate=learning_rate,  # learning rate type\n",
    "    learning_rate_init=learning_rate_init,  # initial learning rate\n",
    "    random_state=random_state,  # random seed for reproducibility\n",
    "    validation_fraction=validation_fraction, # by default\n",
    "    early_stopping=early_stopping,  # stop training when validation score is not improving\n",
    "    \n",
    "    warm_start= True,\n",
    "    max_iter=1,  # number of iterations (epochs) to train the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ada6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = []\n",
    "test_r2 = []\n",
    "train_mae = []\n",
    "test_mae = []\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Disable ConvergenceWarning for the loop\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train_scaled, y_train)  # Trains 1 epoch (due to max_iter=1)\n",
    "    \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "        # Log metrics\n",
    "        train_r2.append(r2_score(y_train, y_train_pred))\n",
    "        test_r2.append(r2_score(y_test, y_test_pred))\n",
    "        train_mae.append(mean_absolute_error(y_train, y_train_pred))\n",
    "        test_mae.append(mean_absolute_error(y_test, y_test_pred))\n",
    "    \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}:\")\n",
    "            print(f\"  Train RÂ²: {train_r2[-1]:.3f} | Test RÂ²: {test_r2[-1]:.3f}\")\n",
    "            print(f\"  Train MAE: {train_mae[-1]:.3f} | Test MAE: {test_mae[-1]:.3f}\")\n",
    "            print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d150d5-74b8-4cb9-9b8d-bf4cb3b82978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training loop\n",
    "def plot_metrics(train_r2, test_r2, train_mae, test_mae):\n",
    "    epochs = range(len(train_r2))  # Since you are logging every epoch\n",
    "\n",
    "    # Create subplots\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Training data plot\n",
    "    plt.subplot(1, 2, 1)  # Define a proper 1x2 grid of subplots\n",
    "\n",
    "    # Plotting RÂ² scores\n",
    "    plt.plot(epochs, train_r2, label='Train RÂ²', color='blue', marker='o', ms=4,  alpha=0.6)\n",
    "    plt.plot(epochs, test_r2, label='Test RÂ²', color='#66c2a5', marker='o', ms=4,  alpha=0.6)\n",
    "    plt.title('RÂ² Scores over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('RÂ² Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plotting MAE\n",
    "    plt.subplot(1, 2, 2)  # Define a proper 1x2 grid of subplots\n",
    "    plt.plot(epochs, train_mae, label='Train MAE', color='blue', linestyle='--', marker='o', ms=4, alpha=0.6)\n",
    "    plt.plot(epochs, test_mae, label='Test MAE', color='#66c2a5', linestyle='--', marker='o', ms=4, alpha=0.6)\n",
    "    plt.title('MAE over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3)  # Adjust spacing between subplots\n",
    "\n",
    "    return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ae631-6bb4-49b1-bfa5-4e98afb7d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plotting function after fitting the model\n",
    "fig = plot_metrics(train_r2, test_r2, train_mae, test_mae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b66cb-f54a-4a61-8b79-59f066f73ae2",
   "metadata": {},
   "source": [
    "### 2.10. Next Steps: Experimentation Ideas\n",
    "Now that you have trained your model, consider trying the following to enhance your exploration:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "    In the **2.3** section, you can explore:\n",
    "    - Trying augmenting ordecreasing your training data. Adjust `test_percentage = 0.2`\n",
    "    - Changing the shuffle seed. `seed = 42`\n",
    "    - Running the model with a different target property.  You can set `target_property =` `barrier` OR `distance` OR `angle`\n",
    "    \n",
    "2. **Hyperparameter Tuning**:\n",
    "    In the **2.5** section, consider:\n",
    "   - Adjusting the learning rate `learning_rate_init=0.0001`. Experiment with learning_rate_init values ranging from 0.0001 to 0.001, and observe in section 2.8. how the training curves change.\n",
    "   - Modifying the number of epochs based on the results observed in section 2.8.\n",
    "   - Implementing learning rate schedules for optimization. Change `learning_rate = 'constant'` to a different strategy. \n",
    "   - Modifying the initiallization seed for the model. `random_state=2025`\n",
    "\n",
    "3. **Model Architecture Adjustments**:\n",
    "    In the **2.5** section, try the following:\n",
    "   - Adding more hidden layers, but be mindful of potential overfitting due to increased parameters.\n",
    "   - Experimenting with different activation functions.\n",
    "\n",
    "4. **Document Your Findings**:\n",
    "   - Keep notes on different configurations and their impacts on model performance!\n",
    "\n",
    "Important Note:\n",
    "\n",
    "Remember that the Jupyter Notebook retains variable values in memory. If you encounter any unexpected results or \"weird\" values, itâ€™s best to rerun the entire page to refresh the environment.\n",
    "\n",
    "Feel free to modify the code in the cells above and re-run the training block. Happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
